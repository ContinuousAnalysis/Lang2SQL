# Production Test Guide â€” v2 Migration

ì´ ë¬¸ì„œëŠ” v2 ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ ìˆ˜í–‰ëœ ëª¨ë“  ë³€ê²½ ì‚¬í•­ì„ **ì‹¤ì œ API í‚¤ì™€ ì‹¤ì œ DB**ë¥¼ ì‚¬ìš©í•´ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì—ì„œ ê²€ì¦í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ì „ì œ ì¡°ê±´

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
uv sync --group dev

# .env ì„¤ì • (ì•„ë˜ ê° ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•  í”„ë¡œë°”ì´ë” í•­ëª©ì„ í™œì„±í™”)
cp .env.example .env
```

ëª¨ë“  Python ìŠ¤ë‹ˆí«ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
cd /path/to/lang2sql
```

---

## 1. LLM í†µí•© â€” 7ê°œ í”„ë¡œë°”ì´ë”

ê° í”„ë¡œë°”ì´ë”ëŠ” `.env`ì—ì„œ í•´ë‹¹ í•­ëª©ì„ ì„¤ì •í•˜ê³  ë…ë¦½ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.

### 1-A. Anthropic

```
# .env
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_LLM_MODEL=claude-sonnet-4-6
```

```python
from lang2sql.integrations.llm.anthropic_ import AnthropicLLM
import os

llm = AnthropicLLM(model="claude-sonnet-4-6", api_key=os.getenv("ANTHROPIC_API_KEY"))
resp = llm.invoke([{"role": "user", "content": "Respond with just 'OK'"}])
assert resp.strip() == "OK", f"Unexpected: {resp}"
print("Anthropic LLM âœ“")
```

**í™•ì¸ í¬ì¸íŠ¸**
- `invoke()` ë°˜í™˜ê°’ì´ `str` íƒ€ì…
- system ë©”ì‹œì§€ê°€ `role: system`ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ Anthropic Messages APIì— ì „ë‹¬ë¨

---

### 1-B. OpenAI

```
# .env
LLM_PROVIDER=openai
OPEN_AI_KEY=sk-proj-...
OPEN_AI_LLM_MODEL=gpt-4o
```

```python
from lang2sql.integrations.llm.openai_ import OpenAILLM
import os

llm = OpenAILLM(model="gpt-4o", api_key=os.getenv("OPEN_AI_KEY"))
resp = llm.invoke([{"role": "user", "content": "Respond with just 'OK'"}])
assert isinstance(resp, str) and len(resp) > 0
print("OpenAI LLM âœ“")
```

---

### 1-C. Azure OpenAI

```
# .env
LLM_PROVIDER=azure
AZURE_OPENAI_LLM_ENDPOINT=https://RESOURCE.openai.azure.com/
AZURE_OPENAI_LLM_KEY=...
AZURE_OPENAI_LLM_MODEL=gpt4o          # Azure deployment name
AZURE_OPENAI_LLM_API_VERSION=2024-07-01-preview
```

```python
from lang2sql.integrations.llm.azure_ import AzureOpenAILLM
import os

llm = AzureOpenAILLM(
    azure_deployment=os.environ["AZURE_OPENAI_LLM_MODEL"],
    azure_endpoint=os.environ["AZURE_OPENAI_LLM_ENDPOINT"],
    api_version=os.getenv("AZURE_OPENAI_LLM_API_VERSION", "2024-07-01-preview"),
    api_key=os.getenv("AZURE_OPENAI_LLM_KEY"),
)
resp = llm.invoke([{"role": "user", "content": "Respond with just 'OK'"}])
assert isinstance(resp, str) and len(resp) > 0
print("Azure OpenAI LLM âœ“")
```

---

### 1-D. Google Gemini

```
# .env
LLM_PROVIDER=gemini
GEMINI_API_KEY=AIza...
GEMINI_LLM_MODEL=gemini-2.0-flash-lite
```

```python
from lang2sql.integrations.llm.gemini_ import GeminiLLM
import os

llm = GeminiLLM(model="gemini-2.0-flash-lite", api_key=os.getenv("GEMINI_API_KEY"))
resp = llm.invoke([{"role": "user", "content": "Respond with just 'OK'"}])
assert isinstance(resp, str) and len(resp) > 0
print("Gemini LLM âœ“")
```

---

### 1-E. AWS Bedrock

```
# .env
LLM_PROVIDER=bedrock
AWS_BEDROCK_LLM_ACCESS_KEY_ID=AKI...
AWS_BEDROCK_LLM_SECRET_ACCESS_KEY=...
AWS_BEDROCK_LLM_REGION=us-east-1
AWS_BEDROCK_LLM_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0
```

```python
from lang2sql.integrations.llm.bedrock_ import BedrockLLM
import os

llm = BedrockLLM(
    model=os.environ["AWS_BEDROCK_LLM_MODEL"],
    aws_access_key_id=os.getenv("AWS_BEDROCK_LLM_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_BEDROCK_LLM_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_BEDROCK_LLM_REGION", "us-east-1"),
)
resp = llm.invoke([{"role": "user", "content": "Respond with just 'OK'"}])
assert isinstance(resp, str) and len(resp) > 0
print("Bedrock LLM âœ“")
```

**í™•ì¸ í¬ì¸íŠ¸**: Bedrock Converse API í¬ë§· â€” `role: system`ì´ `system` ë¸”ë¡ìœ¼ë¡œ ë¶„ë¦¬ë˜ëŠ”ì§€ í™•ì¸

```python
# system ë©”ì‹œì§€ ë¶„ë¦¬ í™•ì¸
resp = llm.invoke([
    {"role": "system", "content": "Always respond in one word."},
    {"role": "user", "content": "Say hello"},
])
assert len(resp.split()) <= 3, f"System prompt not applied: {resp}"
print("Bedrock system message separation âœ“")
```

---

### 1-F. Ollama (ë¡œì»¬)

```
# Ollama ì„œë²„ ì‹¤í–‰ í•„ìš”
# brew install ollama && ollama serve
# ollama pull llama3.2

# .env
LLM_PROVIDER=ollama
OLLAMA_LLM_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=llama3.2
```

```python
from lang2sql.integrations.llm.ollama_ import OllamaLLM
import os

llm = OllamaLLM(
    model=os.environ["OLLAMA_LLM_MODEL"],
    base_url=os.getenv("OLLAMA_LLM_BASE_URL", "http://localhost:11434"),
)
resp = llm.invoke([{"role": "user", "content": "Say hello in one word"}])
assert isinstance(resp, str) and len(resp) > 0
print("Ollama LLM âœ“")
```

---

### 1-G. HuggingFace Inference API

```
# .env
LLM_PROVIDER=huggingface
HUGGING_FACE_LLM_REPO_ID=mistralai/Mistral-7B-Instruct-v0.3
HUGGING_FACE_LLM_API_TOKEN=hf_...
# HUGGING_FACE_LLM_ENDPOINT=https://... (Dedicated Endpoint ì‚¬ìš© ì‹œ)
```

```python
from lang2sql.integrations.llm.huggingface_ import HuggingFaceLLM
import os

llm = HuggingFaceLLM(
    repo_id=os.getenv("HUGGING_FACE_LLM_REPO_ID"),
    api_token=os.getenv("HUGGING_FACE_LLM_API_TOKEN"),
)
resp = llm.invoke([{"role": "user", "content": "Say hello"}])
assert isinstance(resp, str) and len(resp) > 0
print("HuggingFace LLM âœ“")
```

---

## 2. Embedding í†µí•© â€” 6ê°œ í”„ë¡œë°”ì´ë”

### 2-A. OpenAI Embedding

```python
from lang2sql.integrations.embedding.openai_ import OpenAIEmbedding
import os

emb = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key=os.getenv("OPEN_AI_KEY"),
)
vec = emb.embed_query("ì£¼ë¬¸ í…Œì´ë¸”ì˜ ì£¼ë¬¸ ID")
assert isinstance(vec, list) and len(vec) == 1536
print(f"OpenAI Embedding âœ“ (dim={len(vec)})")

vecs = emb.embed_texts(["orders", "customers"])
assert len(vecs) == 2 and len(vecs[0]) == 1536
print("OpenAI batch embed âœ“")
```

---

### 2-B. Azure OpenAI Embedding

```
# .env
EMBEDDING_PROVIDER=azure
AZURE_OPENAI_EMBEDDING_ENDPOINT=https://RESOURCE.openai.azure.com/
AZURE_OPENAI_EMBEDDING_KEY=...
AZURE_OPENAI_EMBEDDING_MODEL=textembeddingada002
AZURE_OPENAI_EMBEDDING_API_VERSION=2023-09-15-preview
```

```python
from lang2sql.integrations.embedding.azure_ import AzureOpenAIEmbedding
import os

emb = AzureOpenAIEmbedding(
    azure_deployment=os.environ["AZURE_OPENAI_EMBEDDING_MODEL"],
    azure_endpoint=os.environ["AZURE_OPENAI_EMBEDDING_ENDPOINT"],
    api_version=os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION"),
    api_key=os.getenv("AZURE_OPENAI_EMBEDDING_KEY"),
)
vec = emb.embed_query("ì£¼ë¬¸ ë°ì´í„°")
assert isinstance(vec, list) and len(vec) > 0
print(f"Azure Embedding âœ“ (dim={len(vec)})")
```

---

### 2-C. Ollama Embedding

```
# .env
EMBEDDING_PROVIDER=ollama
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_EMBEDDING_BASE_URL=http://localhost:11434
```

```python
# ollama pull nomic-embed-text ë¨¼ì € ì‹¤í–‰ í•„ìš”
from lang2sql.integrations.embedding.ollama_ import OllamaEmbedding
import os

emb = OllamaEmbedding(
    model=os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text"),
    base_url=os.getenv("OLLAMA_EMBEDDING_BASE_URL", "http://localhost:11434"),
)
vec = emb.embed_query("test")
assert isinstance(vec, list) and len(vec) > 0
print(f"Ollama Embedding âœ“ (dim={len(vec)})")
```

---

### 2-D. AWS Bedrock Embedding

```
# .env
EMBEDDING_PROVIDER=bedrock
AWS_BEDROCK_EMBEDDING_ACCESS_KEY_ID=...
AWS_BEDROCK_EMBEDDING_SECRET_ACCESS_KEY=...
AWS_BEDROCK_EMBEDDING_REGION=us-east-1
AWS_BEDROCK_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0
```

```python
from lang2sql.integrations.embedding.bedrock_ import BedrockEmbedding
import os

emb = BedrockEmbedding(
    model_id=os.getenv("AWS_BEDROCK_EMBEDDING_MODEL", "amazon.titan-embed-text-v2:0"),
    aws_access_key_id=os.getenv("AWS_BEDROCK_EMBEDDING_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_BEDROCK_EMBEDDING_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_BEDROCK_EMBEDDING_REGION", "us-east-1"),
)
vec = emb.embed_query("ì£¼ë¬¸ ë°ì´í„°")
assert isinstance(vec, list) and len(vec) == 1024   # Titan v2 ê¸°ë³¸ ì°¨ì›
print(f"Bedrock Embedding âœ“ (dim={len(vec)})")
```

---

### 2-E. Google Gemini Embedding

```
# .env
EMBEDDING_PROVIDER=gemini
GEMINI_EMBEDDING_API_KEY=AIza...
EMBEDDING_MODEL=models/embedding-001
```

```python
from lang2sql.integrations.embedding.gemini_ import GeminiEmbedding
import os

emb = GeminiEmbedding(
    model=os.getenv("EMBEDDING_MODEL", "models/embedding-001"),
    api_key=os.getenv("GEMINI_EMBEDDING_API_KEY"),
)
vec = emb.embed_query("ì£¼ë¬¸ ë°ì´í„°")
assert isinstance(vec, list) and len(vec) == 768
print(f"Gemini Embedding âœ“ (dim={len(vec)})")
```

---

### 2-F. HuggingFace Embedding (ë¡œì»¬ ëª¨ë¸)

```
# .env
EMBEDDING_PROVIDER=huggingface
HUGGING_FACE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

```python
# pip install sentence-transformers í•„ìš”
from lang2sql.integrations.embedding.huggingface_ import HuggingFaceEmbedding
import os

emb = HuggingFaceEmbedding(
    model=os.getenv("HUGGING_FACE_EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
)
vec = emb.embed_query("ì£¼ë¬¸ ë°ì´í„°")
assert isinstance(vec, list) and len(vec) == 384   # all-MiniLM-L6-v2 ì°¨ì›
print(f"HuggingFace Embedding âœ“ (dim={len(vec)})")
```

---

## 3. í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ Factory (`build_*_from_env`)

`.env`ì— ì›í•˜ëŠ” í”„ë¡œë°”ì´ë” ì„¤ì •ì„ ë„£ê³  ì•„ë˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
from dotenv import load_dotenv
load_dotenv()

from lang2sql.factory import build_llm_from_env, build_embedding_from_env, build_db_from_env

# LLM
llm = build_llm_from_env()
resp = llm.invoke([{"role": "user", "content": "Say 'ready'"}])
assert isinstance(resp, str)
print(f"build_llm_from_env âœ“ â†’ {resp[:40]}")

# Embedding
emb = build_embedding_from_env()
vec = emb.embed_query("test")
assert isinstance(vec, list) and len(vec) > 0
print(f"build_embedding_from_env âœ“ dim={len(vec)}")

# DB
db = build_db_from_env()
# DB_TYPE=sqlite ì¸ ê²½ìš° ê°„ë‹¨í•œ ì¿¼ë¦¬ ì‹¤í–‰ í™•ì¸
rows = db.execute("SELECT 1 AS val")
assert rows[0]["val"] == 1
print("build_db_from_env âœ“")
```

---

## 4. ê³ ê¸‰ ì»´í¬ë„ŒíŠ¸ â€” ì‹¤ì œ LLM í˜¸ì¶œ

ì•„ë˜ ì˜ˆì œëŠ” Anthropic LLMìœ¼ë¡œ ì‘ì„±ëìœ¼ë‚˜ ì–´ë–¤ í”„ë¡œë°”ì´ë”ë“  ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

### 4-A. QuestionGate

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_llm_from_env
from lang2sql.components.gate.question_gate import QuestionGate
from lang2sql.core.catalog import GateResult

llm = build_llm_from_env()
gate = QuestionGate(llm=llm)

# ì •ìƒ ì¿¼ë¦¬ â†’ suitable=True
result: GateResult = gate("ì§€ë‚œë‹¬ ì£¼ë¬¸ ê±´ìˆ˜ë¥¼ ì•Œë ¤ì¤˜")
assert result.suitable is True, f"suitable False: {result.reason}"
print(f"QuestionGate (suitable) âœ“ â€” reason: {result.reason}")

# ë¹„ì í•© ì¿¼ë¦¬ â†’ suitable=False
result2: GateResult = gate("íšŒì‚¬ ì „ëµ ë³´ê³ ì„œë¥¼ í†µê³„ ëª¨ë¸ë¡œ ë¶„ì„í•´ì¤˜")
assert result2.suitable is False, "Expected unsuitable for data-science request"
print(f"QuestionGate (not suitable) âœ“ â€” reason: {result2.reason}")
```

---

### 4-B. TableSuitabilityEvaluator

```python
from lang2sql.factory import build_llm_from_env
from lang2sql.components.gate.table_suitability import TableSuitabilityEvaluator

llm = build_llm_from_env()
evaluator = TableSuitabilityEvaluator(llm=llm)

catalog = [
    {"name": "orders", "description": "ì£¼ë¬¸ ì •ë³´ í…Œì´ë¸”", "columns": {"order_id": "ì£¼ë¬¸ ID", "amount": "ê¸ˆì•¡", "created_at": "ìƒì„±ì¼"}},
    {"name": "users", "description": "ì‚¬ìš©ì ì •ë³´ í…Œì´ë¸”", "columns": {"user_id": "ìœ ì € ID", "name": "ì´ë¦„"}},
    {"name": "products", "description": "ìƒí’ˆ ì •ë³´ í…Œì´ë¸”", "columns": {"product_id": "ìƒí’ˆ ID", "price": "ê°€ê²©"}},
]

filtered = evaluator("ì§€ë‚œë‹¬ ì£¼ë¬¸ ê±´ìˆ˜", catalog)
# orders í…Œì´ë¸”ì€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨
names = [t["name"] for t in filtered]
assert "orders" in names, f"orders not found in {names}"
print(f"TableSuitabilityEvaluator âœ“ â†’ {names}")
```

---

### 4-C. QuestionProfiler

```python
from lang2sql.factory import build_llm_from_env
from lang2sql.components.enrichment.question_profiler import QuestionProfiler
from lang2sql.core.catalog import QuestionProfile

llm = build_llm_from_env()
profiler = QuestionProfiler(llm=llm)

profile: QuestionProfile = profiler("ì›”ë³„ ì£¼ë¬¸ ê¸ˆì•¡ ì¶”ì´")
assert hasattr(profile, "is_timeseries")
assert hasattr(profile, "intent_type")
print(f"QuestionProfiler âœ“ â€” is_timeseries={profile.is_timeseries}, intent={profile.intent_type}")

profile2: QuestionProfile = profiler("ìƒìœ„ 10ê°œ ê³ ê° ëª©ë¡")
assert hasattr(profile2, "has_ranking")
print(f"QuestionProfiler âœ“ â€” has_ranking={profile2.has_ranking}")
```

---

### 4-D. ContextEnricher

```python
from lang2sql.factory import build_llm_from_env
from lang2sql.components.enrichment.context_enricher import ContextEnricher
from lang2sql.core.catalog import QuestionProfile

llm = build_llm_from_env()
enricher = ContextEnricher(llm=llm)

catalog = [
    {"name": "orders", "description": "ì£¼ë¬¸ ì •ë³´", "columns": {"order_id": "ì£¼ë¬¸ ID", "amount": "ê¸ˆì•¡", "created_at": "ìƒì„±ì¼"}},
]
profile = QuestionProfile(is_aggregation=True, has_filter=True, intent_type="lookup")
enriched = enricher("ì§€ë‚œë‹¬ ì£¼ë¬¸ ê±´ìˆ˜", catalog, profile)

assert isinstance(enriched, str) and len(enriched) > 0
print(f"ContextEnricher âœ“ â€” enriched: {enriched[:100]}")
```

---

## 5. HybridRetriever (BM25 + Vector RRF)

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_embedding_from_env
from lang2sql.components.retrieval.hybrid import HybridRetriever

emb = build_embedding_from_env()

catalog = [
    {"name": "orders", "description": "ì£¼ë¬¸ ì •ë³´ í…Œì´ë¸”", "columns": {"order_id": "ì£¼ë¬¸ ID", "amount": "ê¸ˆì•¡", "created_at": "ìƒì„±ì¼"}},
    {"name": "customers", "description": "ê³ ê° ì •ë³´ í…Œì´ë¸”", "columns": {"customer_id": "ê³ ê° ID", "name": "ì´ë¦„", "email": "ì´ë©”ì¼"}},
    {"name": "products", "description": "ìƒí’ˆ ì •ë³´ í…Œì´ë¸”", "columns": {"product_id": "ìƒí’ˆ ID", "price": "ê°€ê²©"}},
    {"name": "inventory", "description": "ì¬ê³  í…Œì´ë¸”", "columns": {"product_id": "ìƒí’ˆ ID", "stock": "ì¬ê³  ìˆ˜ëŸ‰"}},
]

retriever = HybridRetriever(catalog=catalog, embedding=emb, top_n=2)
result = retriever("ì§€ë‚œë‹¬ ì£¼ë¬¸ ê±´ìˆ˜")

assert len(result.schemas) <= 2
names = [s["name"] for s in result.schemas]
assert "orders" in names, f"orders missing from {names}"
print(f"HybridRetriever âœ“ â†’ schemas={names}")

# ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ context í…ŒìŠ¤íŠ¸
from lang2sql.core.catalog import TextDocument
docs = [TextDocument(id="doc1", content="ì£¼ë¬¸ì€ created_at ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.")]
retriever2 = HybridRetriever(catalog=catalog, embedding=emb, documents=docs, top_n=2)
result2 = retriever2("ì£¼ë¬¸ ë‚ ì§œ ê¸°ì¤€ ì§‘ê³„")
print(f"HybridRetriever with docs âœ“ â€” context={result2.context}")
```

---

## 6. BaselineNL2SQL â€” End-to-End

SQLite ì˜ˆì œ (ê°€ì¥ ë¹ ë¥´ê²Œ ê²€ì¦ ê°€ëŠ¥)

```bash
# í…ŒìŠ¤íŠ¸ DB ì¤€ë¹„
python - <<'EOF'
import sqlite3
conn = sqlite3.connect("test_e2e.db")
conn.execute("CREATE TABLE IF NOT EXISTS orders (order_id INTEGER PRIMARY KEY, amount REAL, created_at TEXT)")
conn.execute("INSERT OR IGNORE INTO orders VALUES (1, 10000, '2024-01-15')")
conn.execute("INSERT OR IGNORE INTO orders VALUES (2, 20000, '2024-01-20')")
conn.execute("INSERT OR IGNORE INTO orders VALUES (3, 15000, '2024-02-05')")
conn.commit()
conn.close()
print("test_e2e.db ìƒì„± ì™„ë£Œ")
EOF
```

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_llm_from_env
from lang2sql.flows import BaselineNL2SQL
from lang2sql.integrations.db.sqlalchemy_ import SQLAlchemyDB

catalog = [
    {
        "name": "orders",
        "description": "ì£¼ë¬¸ ì •ë³´ í…Œì´ë¸”",
        "columns": {"order_id": "ì£¼ë¬¸ ID", "amount": "ì£¼ë¬¸ ê¸ˆì•¡(ì›)", "created_at": "ì£¼ë¬¸ ìƒì„±ì¼(YYYY-MM-DD)"},
    }
]

llm = build_llm_from_env()
db = SQLAlchemyDB("sqlite:///test_e2e.db")

pipeline = BaselineNL2SQL(catalog=catalog, llm=llm, db=db, db_dialect="sqlite")
rows = pipeline.run("ì „ì²´ ì£¼ë¬¸ ê±´ìˆ˜")

assert isinstance(rows, list) and len(rows) > 0
print(f"BaselineNL2SQL âœ“ â€” rows={rows}")
```

---

## 7. EnrichedNL2SQL â€” End-to-End (Full 7-Step Pipeline)

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_llm_from_env, build_embedding_from_env
from lang2sql.flows import EnrichedNL2SQL
from lang2sql.integrations.db.sqlalchemy_ import SQLAlchemyDB
from lang2sql.core.hooks import MemoryHook

catalog = [
    {
        "name": "orders",
        "description": "ì£¼ë¬¸ ì •ë³´ í…Œì´ë¸”. ê³ ê°ì´ ê²°ì œí•œ ì£¼ë¬¸ ê¸°ë¡.",
        "columns": {"order_id": "ì£¼ë¬¸ ID", "amount": "ì£¼ë¬¸ ê¸ˆì•¡(ì›)", "created_at": "ì£¼ë¬¸ ìƒì„±ì¼(YYYY-MM-DD)"},
    }
]

llm = build_llm_from_env()
emb = build_embedding_from_env()
db = SQLAlchemyDB("sqlite:///test_e2e.db")
hook = MemoryHook()

pipeline = EnrichedNL2SQL(
    catalog=catalog,
    llm=llm,
    db=db,
    embedding=emb,
    db_dialect="sqlite",
    gate_enabled=True,
    top_n=3,
    hook=hook,
)

rows = pipeline.run("ì „ì²´ ì£¼ë¬¸ ê±´ìˆ˜ë¥¼ ì•Œë ¤ì¤˜")
assert isinstance(rows, list) and len(rows) > 0
print(f"EnrichedNL2SQL âœ“ â€” rows={rows}")

# Hook ì´ë²¤íŠ¸ í™•ì¸ (QuestionGate ~ SQLExecutorê¹Œì§€ 7ë‹¨ê³„ ì´ë²¤íŠ¸ ë°œìƒ í™•ì¸)
components = {e.component for e in hook.events}
print(f"  â†’ ì‹¤í–‰ëœ ì»´í¬ë„ŒíŠ¸: {components}")
assert "QuestionGate" in components
assert "HybridRetriever" in components
assert "SQLGenerator" in components
assert "SQLExecutor" in components
print("  â†’ Hook ì´ë²¤íŠ¸ âœ“")
```

### 7-A. QuestionGate â€” ContractError ë°œìƒ í™•ì¸

```python
from lang2sql.core.exceptions import ContractError
import pytest

try:
    pipeline.run("ìš°ë¦¬ íšŒì‚¬ ë§ˆì¼€íŒ… ì „ëµì„ ML ëª¨ë¸ë¡œ ì˜ˆì¸¡í•´ì¤˜")
    print("WARNING: ContractErrorê°€ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤")
except ContractError as e:
    print(f"ContractError âœ“ â€” {e}")
```

### 7-B. gate_enabled=False â€” Gate ë¹„í™œì„±í™” í™•ì¸

```python
pipeline_no_gate = EnrichedNL2SQL(
    catalog=catalog, llm=llm, db=db, embedding=emb,
    db_dialect="sqlite", gate_enabled=False,
)
rows2 = pipeline_no_gate.run("ì „ì²´ ì£¼ë¬¸ ê¸ˆì•¡ í•©ê³„")
assert isinstance(rows2, list)
print(f"EnrichedNL2SQL (no gate) âœ“ â€” rows={rows2}")
```

---

## 8. CLI ëª…ë ¹ì–´

`.env`ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ëœ ìƒíƒœì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

### 8-A. Baseline í”Œë¡œìš°

```bash
lang2sql query "ì „ì²´ ì£¼ë¬¸ ê±´ìˆ˜" \
  --flow baseline \
  --dialect sqlite
```

**ì˜ˆìƒ ì¶œë ¥**: JSON ë°°ì—´ (ê²°ê³¼ í–‰) ë˜ëŠ” `(ê²°ê³¼ ì—†ìŒ)`

---

### 8-B. Enriched í”Œë¡œìš°

```bash
lang2sql query "ì§€ë‚œ 1ì›” ì£¼ë¬¸ ê¸ˆì•¡ í•©ê³„" \
  --flow enriched \
  --dialect sqlite \
  --top-n 3
```

---

### 8-C. Gate ë¹„í™œì„±í™”

```bash
lang2sql query "ì „ì²´ ì£¼ë¬¸ ê±´ìˆ˜" \
  --flow enriched \
  --no-gate \
  --dialect sqlite
```

---

### 8-D. ì—ëŸ¬ ì¼€ì´ìŠ¤ í™•ì¸

```bash
# LLM_PROVIDERë¥¼ ì˜ëª»ëœ ê°’ìœ¼ë¡œ ì„¤ì •í•œ ê²½ìš°
LLM_PROVIDER=unknown lang2sql query "test"
# ì˜ˆìƒ: ValueError: Unknown LLM_PROVIDER: 'unknown'
```

---

## 9. DataHub ì¹´íƒˆë¡œê·¸ ë¸Œë¦¿ì§€

> DataHub GMS ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

```
# .env
DATAHUB_SERVER=http://localhost:8080
```

```python
import os
from dotenv import load_dotenv; load_dotenv()
from lang2sql.integrations.catalog.datahub_ import DataHubCatalogLoader

loader = DataHubCatalogLoader(gms_server=os.getenv("DATAHUB_SERVER", "http://localhost:8080"))
catalog = loader.load()

assert isinstance(catalog, list)
assert len(catalog) > 0, "DataHubì— í…Œì´ë¸”ì´ í•˜ë‚˜ ì´ìƒ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤"

first = catalog[0]
assert "name" in first and "description" in first and "columns" in first
print(f"DataHubCatalogLoader âœ“ â€” {len(catalog)}ê°œ í…Œì´ë¸” ë¡œë“œ")
print(f"  ì²« ë²ˆì§¸: name={first['name']}, columns={list(first['columns'].keys())[:5]}")
```

### DataHub Catalog â†’ EnrichedNL2SQL ì—°ë™

```python
from lang2sql.factory import build_llm_from_env, build_embedding_from_env, build_db_from_env

llm = build_llm_from_env()
emb = build_embedding_from_env()
db = build_db_from_env()
pipeline = EnrichedNL2SQL(
    catalog=catalog,   # DataHubì—ì„œ ë¡œë“œí•œ catalog ì‚¬ìš©
    llm=llm, db=db, embedding=emb,
    gate_enabled=True,
)
rows = pipeline.run("ìœ ë‹ˆí¬í•œ ìœ ì € ìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•´ì¤˜")
print(f"DataHub catalog + EnrichedNL2SQL âœ“ â€” {rows}")
```

---

## 10. FAISSVectorStore (v2 ë²¡í„° ìŠ¤í† ì–´)

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_embedding_from_env
from lang2sql.integrations.vectorstore.faiss_ import FAISSVectorStore

emb = build_embedding_from_env()

# ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥
texts = ["ì£¼ë¬¸ í…Œì´ë¸”: ê³ ê° ì£¼ë¬¸ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤", "ê³ ê° í…Œì´ë¸”: íšŒì› ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤"]
vectors = emb.embed_texts(texts)

store = FAISSVectorStore(index_path="/tmp/test_faiss.idx")
store.upsert(ids=["doc0", "doc1"], vectors=vectors)

# ê²€ìƒ‰
query_vec = emb.embed_query("ì£¼ë¬¸ ì •ë³´")
results = store.search(query_vec, k=2)

assert len(results) > 0
assert results[0][0] in ["doc0", "doc1"]
print(f"FAISSVectorStore âœ“ â€” top result: {results[0]}")

# ì €ì¥/ë¡œë“œ
store.save()
loaded = FAISSVectorStore.load("/tmp/test_faiss.idx")
results2 = loaded.search(query_vec, k=1)
assert results2[0][0] == results[0][0]
print("FAISSVectorStore save/load âœ“")
```

---

## 11. Streamlit UI ìˆ˜ë™ ê²€ì¦

```bash
lang2sql run-streamlit
# ë˜ëŠ”
streamlit run interface/streamlit_app.py
```

### ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | í™•ì¸ ë°©ë²• | í†µê³¼ ì¡°ê±´ |
|------|-----------|-----------|
| í™ˆ í˜ì´ì§€ | `http://localhost:8501` ì ‘ì† | ì—ëŸ¬ ì—†ì´ ë¡œë“œ |
| Lang2SQL â€” Baseline | ì›Œí¬í”Œë¡œìš° ì²´í¬ë°•ìŠ¤ í•´ì œ â†’ "ì¿¼ë¦¬ ì‹¤í–‰" | ê²°ê³¼ í…Œì´ë¸” ë Œë”ë§ |
| Lang2SQL â€” Enriched | ì²´í¬ë°•ìŠ¤ ì„ íƒ â†’ "ì¿¼ë¦¬ ì‹¤í–‰" | ê²°ê³¼ í…Œì´ë¸” ë Œë”ë§ |
| Dialect ì„ íƒ | `sqlite` â†’ `postgresql` ì „í™˜ | ë“œë¡­ë‹¤ìš´ ë³€ê²½ ë°˜ì˜ |
| ì˜¤ë¥˜ í‘œì‹œ | ì—°ê²° ë¶ˆê°€ DB ì„¤ì • í›„ ì‹¤í–‰ | `st.error()` ì—ëŸ¬ ë°•ìŠ¤ |
| ChatBot í˜ì´ì§€ | `ğŸ¤– ChatBot` íƒ­ í´ë¦­ | ì—ëŸ¬ ì—†ì´ ë¡œë“œ |
| ì„¤ì • í˜ì´ì§€ | `âš™ï¸ ì„¤ì •` íƒ­ í´ë¦­ | ì—ëŸ¬ ì—†ì´ ë¡œë“œ |
| Graph Builder í˜ì´ì§€ ì—†ìŒ | ë„¤ë¹„ê²Œì´ì…˜ íƒ­ í™•ì¸ | íƒ­ ëª©ë¡ì— ì—†ì–´ì•¼ í•¨ |

---

## 12. ChatBot â€” LangGraph + ìˆ˜ì •ëœ `search_database_tables`

> `DATAHUB_SERVER`ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. DataHub ì—†ì´ ê²€ìƒ‰ ì‹œ ì—ëŸ¬ ì‘ë‹µ(`{"error": True, ...}`)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
import os
from dotenv import load_dotenv; load_dotenv()

# 12-A. ëª¨ë“ˆ ì„í¬íŠ¸ ë¬´ê²°ì„± í™•ì¸ (í•µì‹¬: retrieval.py ì‚­ì œ ì´í›„ ì„í¬íŠ¸ ì„±ê³µ í™•ì¸)
from utils.llm.tools import search_database_tables, get_glossary_terms, get_query_examples
print("utils.llm.tools import âœ“")

from utils.llm.chatbot import ChatBot
print("utils.llm.chatbot import âœ“")

# 12-B. ChatBot ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
bot = ChatBot(
    openai_api_key=os.getenv("OPEN_AI_KEY"),
    model_name="gpt-4o-mini",
    gms_server=os.getenv("DATAHUB_SERVER", "http://localhost:8080"),
)
print("ChatBot instance âœ“")

# 12-C. ê¸°ë³¸ ëŒ€í™” í…ŒìŠ¤íŠ¸
result = bot.chat("ì•ˆë…•í•˜ì„¸ìš”", thread_id="test-001")
last_msg = result["messages"][-1]
assert hasattr(last_msg, "content") and len(last_msg.content) > 0
print(f"ChatBot.chat() âœ“ â€” ì‘ë‹µ: {last_msg.content[:60]}")
```

### 12-D. `search_database_tables` ì§ì ‘ í˜¸ì¶œ (DataHub ì—°ê²° ì‹œ)

```python
# DataHubê°€ ì—°ê²°ëœ í™˜ê²½ì—ì„œë§Œ ì„±ê³µì ì¸ ê²°ê³¼ ë°˜í™˜
result = search_database_tables.invoke({
    "query": "ì£¼ë¬¸ í…Œì´ë¸”",
    "top_n": 3
})
# DataHub ì—°ê²° ì„±ê³µ ì‹œ: {"orders": {"table_description": "...", ...}, ...}
# DataHub ì—°ê²° ì‹¤íŒ¨ ì‹œ: {"error": True, "message": "..."}
print(f"search_database_tables âœ“ â€” result keys: {list(result.keys())}")
```

---

## 13. ë ˆê±°ì‹œ ì •ë¦¬ (ì‚­ì œ í™•ì¸)

ì•„ë˜ ëª¨ë“ˆë“¤ì€ ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. **ì„í¬íŠ¸ ì‹œ ì—ëŸ¬ ë°œìƒì´ ì •ìƒ**ì…ë‹ˆë‹¤.

```python
import importlib, sys

deleted_modules = [
    "engine",
    "engine.query_executor",
    "utils.llm.core.factory",
    "utils.llm.chains",
    "utils.llm.retrieval",
    "utils.llm.vectordb",
    "utils.llm.graph_utils",
    "utils.llm.output_schema",
]

for mod in deleted_modules:
    try:
        importlib.import_module(mod)
        print(f"WARNING: {mod} â€” ì‚­ì œë˜ì—ˆì–´ì•¼ í•˜ì§€ë§Œ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤")
    except (ImportError, ModuleNotFoundError):
        print(f"âœ“ {mod} ì‚­ì œ í™•ì¸")
```

---

## 14. ì „ì²´ íšŒê·€ í…ŒìŠ¤íŠ¸

```bash
# ìœ ë‹› í…ŒìŠ¤íŠ¸ ì „ì²´ ì‹¤í–‰ (145 passed, 6 skipped ì˜ˆìƒ)
pytest tests/ -v --tb=short

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
pytest tests/ --cov=src/lang2sql --cov-report=term-missing
```

**ì˜ˆìƒ ê²°ê³¼**: 145 passed, 6 skipped (pgvector ê´€ë ¨ â€” ì‹¤ì œ PostgreSQL ì—†ì´ëŠ” skip)

---

## 15. DB ì»¤ë„¥í„° ê²€ì¦

ì‚¬ìš©í•˜ëŠ” DBì— ë§ê²Œ `.env`ë¥¼ ì„¤ì •í•˜ê³  ì•„ë˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
from dotenv import load_dotenv; load_dotenv()
from lang2sql.factory import build_db_from_env

db = build_db_from_env()

# ì‹¤ì œ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
rows = db.execute("SELECT COUNT(*) AS cnt FROM ì‹¤ì œ_í…Œì´ë¸”ëª…")
assert isinstance(rows, list) and "cnt" in rows[0]
print(f"DB ì—°ê²° âœ“ â€” count={rows[0]['cnt']}")
```

### ì§€ì› DB ëª©ë¡ ë° `.env` í‚¤

| DB | `DB_TYPE` | í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ |
|----|-----------|---------------|
| SQLite | `sqlite` | `SQLITE_PATH` |
| PostgreSQL | `postgresql` | `POSTGRESQL_HOST/PORT/USER/PASSWORD/DATABASE` |
| MySQL | `mysql` | `MYSQL_HOST/PORT/USER/PASSWORD/DATABASE` |
| MariaDB | `mariadb` | `MARIADB_HOST/PORT/USER/PASSWORD/DATABASE` |
| DuckDB | `duckdb` | `DUCKDB_PATH` |
| ClickHouse | `clickhouse` | `CLICKHOUSE_HOST/PORT/USER/PASSWORD/DATABASE` |
| Snowflake | `snowflake` | `SNOWFLAKE_USER/PASSWORD/ACCOUNT` |
| Oracle | `oracle` | `ORACLE_HOST/PORT/USER/PASSWORD/SERVICE_NAME` |

---

## ë¹ ë¥¸ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ `smoke_test.py`ë¡œ ì €ì¥ í›„ ì‹¤í–‰í•˜ë©´ ê°€ì¥ ì¤‘ìš”í•œ ê²½ë¡œë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
"""
smoke_test.py â€” í•µì‹¬ ê²½ë¡œ ë¹ ë¥¸ ê²€ì¦ (Anthropic + SQLite ê¸°ì¤€)
ì‹¤í–‰: python smoke_test.py
"""

import os
import sqlite3

from dotenv import load_dotenv
load_dotenv()

print("=" * 50)
print("Lang2SQL v2 Smoke Test")
print("=" * 50)

# 1. í…ŒìŠ¤íŠ¸ DB
print("\n[1] SQLite DB ì¤€ë¹„")
conn = sqlite3.connect("/tmp/smoke.db")
conn.execute("CREATE TABLE IF NOT EXISTS orders (id INT, amount REAL, created_at TEXT)")
conn.execute("DELETE FROM orders")
conn.executemany("INSERT INTO orders VALUES (?,?,?)", [
    (1, 10000, "2024-01-10"), (2, 20000, "2024-01-20"), (3, 15000, "2024-02-01")
])
conn.commit(); conn.close()
print("  âœ“ /tmp/smoke.db")

# 2. Factory
print("\n[2] Factory ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
from lang2sql.factory import build_llm_from_env, build_embedding_from_env, build_db_from_env
llm = build_llm_from_env()
emb = build_embedding_from_env()
db  = build_db_from_env() if os.getenv("DB_TYPE") else None
print(f"  âœ“ LLM={llm.__class__.__name__}, Embedding={emb.__class__.__name__}")

# 3. LLM í†µì‹ 
print("\n[3] LLM í˜¸ì¶œ")
resp = llm.invoke([{"role": "user", "content": "Respond with OK"}])
assert isinstance(resp, str) and len(resp) > 0
print(f"  âœ“ response={resp[:30]}")

# 4. BaselineNL2SQL
print("\n[4] BaselineNL2SQL")
from lang2sql.flows import BaselineNL2SQL
from lang2sql.integrations.db.sqlalchemy_ import SQLAlchemyDB

catalog = [{"name": "orders", "description": "ì£¼ë¬¸ í…Œì´ë¸”", "columns": {"id": "ì£¼ë¬¸ ID", "amount": "ê¸ˆì•¡", "created_at": "ìƒì„±ì¼"}}]
pipe_base = BaselineNL2SQL(catalog=catalog, llm=llm, db=SQLAlchemyDB("sqlite:////tmp/smoke.db"), db_dialect="sqlite")
rows = pipe_base.run("ì „ì²´ ì£¼ë¬¸ ê±´ìˆ˜")
assert isinstance(rows, list) and len(rows) > 0
print(f"  âœ“ rows={rows}")

# 5. EnrichedNL2SQL
print("\n[5] EnrichedNL2SQL")
from lang2sql.flows import EnrichedNL2SQL

pipe_rich = EnrichedNL2SQL(
    catalog=catalog, llm=llm, embedding=emb,
    db=SQLAlchemyDB("sqlite:////tmp/smoke.db"),
    db_dialect="sqlite", gate_enabled=True,
)
rows2 = pipe_rich.run("ì£¼ë¬¸ ì´ ê±´ìˆ˜")
assert isinstance(rows2, list) and len(rows2) > 0
print(f"  âœ“ rows={rows2}")

# 6. ì‚­ì œ í™•ì¸
print("\n[6] ì‚­ì œëœ ë ˆê±°ì‹œ ëª¨ë“ˆ í™•ì¸")
import importlib
for m in ["utils.llm.retrieval", "utils.llm.vectordb", "utils.llm.chains"]:
    try:
        importlib.import_module(m)
        print(f"  WARNING: {m} ì¡´ì¬ â€” ì‚­ì œ í•„ìš”")
    except (ImportError, ModuleNotFoundError):
        print(f"  âœ“ {m} ì‚­ì œë¨")

print("\n" + "=" * 50)
print("Smoke Test ì™„ë£Œ")
print("=" * 50)
```

```bash
python smoke_test.py
```
